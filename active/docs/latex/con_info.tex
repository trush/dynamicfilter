This desc. is old and some of the variable names may no longer match, but the algorithm described is still the same Our consensus metric is Complicated. For each IP pair chosen, we do the following We gather (N\+U\+M\+\_\+\+C\+E\+R\+T\+A\+I\+N\+\_\+\+V\+O\+T\+ES) votes on the chosen IP pair To take \char`\"{}consensus\char`\"{} we generate a beta distribution from the number of (y/n) votes then intigrate over it from zero to (D\+E\+C\+I\+S\+I\+O\+N\+\_\+\+T\+H\+R\+E\+S\+H\+O\+LD) if the probability area is less than (U\+N\+C\+E\+R\+T\+A\+I\+N\+T\+Y\+\_\+\+T\+H\+R\+E\+S\+H\+O\+LD) then we have consensus else we gather more votes This is repeated until one of several conditions is met 1 -\/ We reach consensus (naturally(\+Bayes)) 2 -\/ The total number of gathered votes is equal to (C\+U\+T\+\_\+\+O\+FF) 3 -\/ The number of either (yes)s or (no)s on their own is equal to (S\+I\+N\+G\+L\+E\+\_\+\+V\+O\+T\+E\+\_\+\+C\+U\+T\+O\+FF) If either cond. (2$\vert$3) we take a simple majority vote

General Consensus

Maximum number of votes to ask for before using Majority Vote as backup metric Only rather ambiguous IP pairs should ever actually reach this limit Recomended value (21 for real data) \#\+T\+O\+DO test more stuff on synth data

Number of votes for a single result (Y/N) before calling that the winner \#\+T\+O\+DO remove this! This should be depricated soon if you\textquotesingle{}re reading this, Jake forgot to take this variable out or was lazy

Bayes\+: The Bayes portion of the alg is weird and bayesian we assume no prior knowledge of the IP pair and thus that there is an even likelyhood of it being either true or false. In the bayesian world we represent everything as a distribution of probability. We use a beta-\/distribution \mbox{[}\href{https://en.wikipedia.org/wiki/Beta_distribution}{\tt https\+://en.\+wikipedia.\+org/wiki/\+Beta\+\_\+distribution}\mbox{]} to represent the distribution of our probability. the beta-\/distribution has two parameters which govern its shape, (a\&b). we start with both at 1 which is a uniform flat distribution. These a and b represent the number of votes for either yes or no on a given IP pair where their values should always be 1 more than the number of votes for each catagory. To take consensus, we build our distribution and integrate over it from zero to D\+E\+C\+I\+S\+I\+O\+N\+\_\+\+T\+H\+R\+E\+S\+H\+O\+LD. If the total area in that area is less than the U\+N\+C\+E\+R\+T\+A\+I\+N\+T\+Y\+\_\+\+T\+H\+R\+E\+S\+H\+O\+LD, we have reached consensus. The motivation is this. The integration is asking the question\+: \char`\"{}\+With the data we have right now, what\textquotesingle{}s the probability that the true \mbox{[}...\mbox{]}
     probability is between 0 and (e.\+g.) 0.\+5.\char`\"{} (Our IP pair has some true ratio of yes votes to no votes.) If the probability of the ratio being within that range is small enough, (smaller than U\+N\+C\+E\+R\+T\+A\+I\+N\+T\+Y\+\_\+\+T\+H\+R\+E\+S\+H\+O\+LD) we can conclude that the true ratio must be larger than that. If the probability is low enough, and the D\+E\+C\+I\+S\+I\+O\+N\+\_\+\+T\+H\+R\+E\+S\+H\+O\+LD chosen correctly, we can say that we have determined the ratio, and thus know the \char`\"{}true\char`\"{} answer to our question.

Adaptive Consensus Our algorithm can attempt to \char`\"{}\+Learn\char`\"{} what a good consensus alg. looks like by looking at the IP pairs which reach Completion (Total Number of tasks, \char`\"{}\+Location\char`\"{}, etc.) Below are the configurations the adaptability. This section is still very much in progress and subject to much change 